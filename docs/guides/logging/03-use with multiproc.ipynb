{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dce8569",
   "metadata": {},
   "source": [
    "# Using `LoguruInitializer` with multiprocessing\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to properly use the `LoguruInitializer` with multiprocessing tasks. We'll cover two main approaches:\n",
    "\n",
    "1. **Creating sub-processes manually** - Where each worker process initializes its own logger\n",
    "2. **Working with a process pool** - Where we use a pool initializer to set up logging for all worker processes\n",
    "\n",
    "The key concepts:\n",
    "- Each worker process needs its own logger initialization\n",
    "- Use `logger.complete()` to ensure all log messages are flushed\n",
    "- Be aware of platform-specific considerations (Linux vs Windows) and start methods (fork vs spawn/forkserver)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d7218",
   "metadata": {},
   "source": [
    "## Example 1: Creating Sub-processes Manually\n",
    "\n",
    "In this example, we create worker processes manually using `multiprocessing.Process`. Each worker process initializes its own logger instance.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Each worker process calls `LoguruInitalizer()` independently\n",
    "- Use `logger.complete()` to ensure all log messages are flushed before the process terminates\n",
    "- Each process can have its own log file (using the PID in the filename)\n",
    "\n",
    "**Compatibility:**\n",
    "\n",
    "- Works seamlessly with all start methods (spawn, fork, forkserver)\n",
    "- However, because Jupyter notebooks requires `fork`, users should copy the codes to a separate Python script and run it outside of Jupyter for full compatibility with all start methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc75499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from alpenstock.logging import LoguruInitalizer, logger\n",
    "import multiprocessing as mp\n",
    "from pathlib import Path\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from random import random\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bd5181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_proc_task(tmp_dir: str):\n",
    "    \"\"\"\n",
    "    Worker function that will be executed in a separate process.\n",
    "    Each worker initializes its own logger and logs to a separate file.\n",
    "    \"\"\"\n",
    "    tmp_dir = Path(tmp_dir)\n",
    "    pid = os.getpid()\n",
    "\n",
    "    # Initialize logger once per worker process\n",
    "    (\n",
    "        LoguruInitalizer()\n",
    "        .preset_full()  # Use preset_full to be distinct from the main process\n",
    "        .serialize_to_file(tmp_dir / f\"log-{pid}.log\")\n",
    "        .initialize(on_reinitialize=\"abort\")\n",
    "        # If the logger is already initialized in a process, `abort` will raise\n",
    "        # an error to let you know.\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Worker {pid} started\")\n",
    "    time.sleep(random() * 0.05)\n",
    "    logger.debug(f\"Worker {pid} is doing some work\")\n",
    "    time.sleep(random() * 0.05)\n",
    "    logger.info(f\"Worker {pid} finished work\")\n",
    "\n",
    "    # Without this line, some log messages may be missing\n",
    "    logger.complete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db37e5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sub_proc_manually():\n",
    "    \"\"\"\n",
    "    Main function that creates and manages worker processes manually.\n",
    "    \"\"\"\n",
    "    # Initialize logger in the main process\n",
    "    LoguruInitalizer().preset_brief().initialize(on_reinitialize=\"abort\")\n",
    "\n",
    "    tmp_dir = tempfile.mkdtemp(prefix=\"alpenstock-\")\n",
    "    logger.info(f\"Temporary directory created: {tmp_dir}\")\n",
    "\n",
    "    # Compatible with spawn, fork and forkserver methods\n",
    "    mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "    processes = []\n",
    "    for _ in range(4):  # Create 4 worker processes\n",
    "        p = mp.Process(target=sub_proc_task, args=(tmp_dir,))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()  # Wait for all processes to finish\n",
    "\n",
    "    logger.info(\"All worker processes have completed.\")\n",
    "    \n",
    "    # Show the log files that were created\n",
    "    log_files = list(Path(tmp_dir).glob(\"*\"))\n",
    "    logger.info(f\"Log files created: {[f.name for f in log_files]}\")\n",
    "    \n",
    "    return tmp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ea2c574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mTemporary directory created: /tmp/alpenstock-dcjjoufa\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:19\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134364 started\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:19\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134365 started\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:19\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134366 started\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:19\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134368 started\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[38;2;159;204;224m\u001b[1mDEBUG  \u001b[0m\u001b[38;2;159;204;224m\u001b[0m|\u001b[38;2;159;204;224m\u001b[22m2509924454:sub_proc_task:21\u001b[0m\u001b[38;2;159;204;224m\u001b[0m|MainThread|\u001b[38;2;159;204;224m\u001b[22mWorker 134364 is doing some work\u001b[0m\u001b[38;2;159;204;224m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[38;2;159;204;224m\u001b[1mDEBUG  \u001b[0m\u001b[38;2;159;204;224m\u001b[0m|\u001b[38;2;159;204;224m\u001b[22m2509924454:sub_proc_task:21\u001b[0m\u001b[38;2;159;204;224m\u001b[0m|MainThread|\u001b[38;2;159;204;224m\u001b[22mWorker 134365 is doing some work\u001b[0m\u001b[38;2;159;204;224m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[38;2;159;204;224m\u001b[1mDEBUG  \u001b[0m\u001b[38;2;159;204;224m\u001b[0m|\u001b[38;2;159;204;224m\u001b[22m2509924454:sub_proc_task:21\u001b[0m\u001b[38;2;159;204;224m\u001b[0m|MainThread|\u001b[38;2;159;204;224m\u001b[22mWorker 134368 is doing some work\u001b[0m\u001b[38;2;159;204;224m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[38;2;159;204;224m\u001b[1mDEBUG  \u001b[0m\u001b[38;2;159;204;224m\u001b[0m|\u001b[38;2;159;204;224m\u001b[22m2509924454:sub_proc_task:21\u001b[0m\u001b[38;2;159;204;224m\u001b[0m|MainThread|\u001b[38;2;159;204;224m\u001b[22mWorker 134366 is doing some work\u001b[0m\u001b[38;2;159;204;224m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:23\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134365 finished work\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:23\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134364 finished work\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:23\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134368 finished work\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22m2509924454:sub_proc_task:23\u001b[0m\u001b[0m|MainThread|\u001b[22mWorker 134366 finished work\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mAll worker processes have completed.\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mLog files created: ['log-134364.log', 'log-134365.log', 'log-134366.log', 'log-134368.log']\u001b[0m\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/alpenstock-dcjjoufa'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run Example 1: Manual process creation\n",
    "create_sub_proc_manually()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd81fa5",
   "metadata": {},
   "source": [
    "## Example 2: Working with a Process Pool\n",
    "\n",
    "In this approach, we use `multiprocessing.Pool` to manage a pool of worker processes. This is more efficient for many small tasks.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "- Use a pool initializer function to set up logging for all worker processes\n",
    "- **DO NOT** initialize the logger in the task function itself, as pool processes may be reused\n",
    "- Be aware of platform-specific considerations:\n",
    "  - **Linux + spawn/forkserver + mp.Pool**: May require disabling enqueue mode to avoid semaphore leaks\n",
    "  - **Windows + spawn + mp.Pool**: Works without issues\n",
    "  - **Linux + fork + mp.Pool**: Works without issues (fork is preferred on Linux)\n",
    "\n",
    "**Compatibility:**\n",
    "\n",
    "The combination of Linux + spawn/forkserver + mp.Pool can lead to internal semaphore leaks inside the `loguru`. Luckily, this is uncommon in practice since fork is the preferred method on Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ecfec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_task(x):\n",
    "    \"\"\"\n",
    "    Task function that will be executed by worker processes in the pool.\n",
    "    \n",
    "    IMPORTANT: DO NOT initialize logger here, as a process pool may reuse \n",
    "    the same worker process for multiple tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(0.05 + 0.05 * random())  # Simulate some work\n",
    "\n",
    "    pid = os.getpid()\n",
    "    logger.info(f\"Worker {pid} processing {x}\")\n",
    "\n",
    "    logger.complete()  # Ensure all log messages are flushed\n",
    "    return x * x  # Example computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e6ecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_initializer():\n",
    "    \"\"\"\n",
    "    Initializer function that sets up logging for each worker process in the pool.\n",
    "    This is called once per worker process when the pool is created.\n",
    "    \"\"\"\n",
    "    # Initialize logger here (once per worker process)\n",
    "    (\n",
    "        LoguruInitalizer()\n",
    "        .preset_brief()\n",
    "        # Disable enqueue for the combination of `Linux` + `spawn`/`forkserver`\n",
    "        # + `mp.Pool`, or the internal semaphores of Loguru will not be cleaned\n",
    "        # up properly, and a warning message will be printed at shutdown like\n",
    "        # this:\n",
    "        #\n",
    "        #   resource_tracker.py:301: UserWarning: resource_tracker: There appear\n",
    "        #   to be 8 leaked semaphore objects to clean up at shutdown: ...\n",
    "        #\n",
    "        # .set_enqueue(False)\n",
    "        .initialize(on_reinitialize=\"abort\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b17ef5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_with_proc_pool():\n",
    "    \"\"\"\n",
    "    Main function that demonstrates using LoguruInitializer with a process pool.\n",
    "    \"\"\"\n",
    "    # Initialize logger in the main process\n",
    "    (\n",
    "        LoguruInitalizer()\n",
    "        .preset_brief()\n",
    "        .initialize(on_reinitialize=\"overwrite\")\n",
    "        # Because we have initailized the logger in the previous example,\n",
    "        # so we can use `overwrite` here to allow reinitialization.\n",
    "    )\n",
    "\n",
    "    # You can force a specific start method for testing\n",
    "    mp.set_start_method(\"fork\", force=True)\n",
    "\n",
    "    with mp.Pool(\n",
    "        processes=4,\n",
    "        initializer=worker_initializer,  # Register the initializer function here\n",
    "    ) as pool:\n",
    "        # Process a range of numbers\n",
    "        results = pool.map(pool_task, range(10))\n",
    "        logger.info(f\"Results: {results}\")\n",
    "\n",
    "    logger.info(\"Process pool has completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15004186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134381 processing 3\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134378 processing 1\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134377 processing 0\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134379 processing 2\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134378 processing 5\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134377 processing 6\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134381 processing 4\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134379 processing 7\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134378 processing 8\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mWorker 134377 processing 9\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mResults: [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\u001b[0m\u001b[0m\n",
      "\u001b[32m07-07 13:58:32\u001b[0m|\u001b[1mINFO   \u001b[0m\u001b[0m|\u001b[22mProcess pool has completed.\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run Example 2: Process pool demo\n",
    "demo_with_proc_pool()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
